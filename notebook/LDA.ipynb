{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alanliu99/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alanliu99/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 1.15 s, total: 12.4 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#os.chdir('../')\n",
    "business = pd.read_csv('data/business.csv')\n",
    "#review = pd.read_csv('test/testdata/test_review.csv')\n",
    "las_vagas_reviews = pd.read_csv('data/Las_Vegas_review.csv')\n",
    "#reviews = pd.read_csv('data/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_vagas_reviews.text= las_vagas_reviews.text.fillna(\"\")\n",
    "rest = las_vagas_reviews[las_vagas_reviews.text.apply(lambda x: len(x.split())) > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1000_id = np.unique(rest.business_id)[0:2000]\n",
    "test_df = rest[rest.business_id.isin(test_1000_id)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 25s, sys: 4.67 s, total: 5min 30s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Lemmatize, remove punctuations and symbols\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "sent_pre1 = test_df['text'].apply(lambda a: [lmtzr.lemmatize(word) for word in tokenizer.tokenize(a.lower())]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pre2 = sent_pre1.apply(lambda a:[word for word in a if ((re.search(r'[^\\w\\s]', word) is None)|(len(word)>1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_W = set([lmtzr.lemmatize(word) for word in stopwords.words('english')])\n",
    "tokenised_sentences =  sent_pre2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vocabulory\n",
    "vocab = []\n",
    "for sent in tokenised_sentences:\n",
    "    for word in sent:\n",
    "        vocab.append(word)\n",
    "#Remove stopwords for LDA\n",
    "vocab = set(vocab) - st_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Bag of Words\n",
    "vectorizer = CountVectorizer(binary=False\n",
    "                             , vocabulary = list(vocab)\n",
    "                             , min_df = 3, max_df=.8)\n",
    "vectorizer = vectorizer.fit(tokenised_sentences.apply(lambda a: \" \".join(a)))\n",
    "\n",
    "BoW=vectorizer.transform(tokenised_sentences.apply(lambda a: \" \".join(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(tokenised_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenised_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 16.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Set number of LDA components\n",
    "n_components = 25\n",
    "#Number of words for showing LDA results\n",
    "n_top_words = 20\n",
    "\n",
    "#Helper function showing LDA topic words\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "#lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=n_components,\n",
    "#                                       id2word=dictionary,\n",
    "#                                       offset = 50,\n",
    "#                                       random_state = 0,\n",
    "#                                       passes=2,\n",
    "#                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run LDA on bag of words of review text\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 3min 39s, sys: 10min 31s, total: 4h 14min 10s\n",
      "Wall time: 2h 37min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BoW_lda = lda.fit_transform(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -161246370.87889653\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda.score(BoW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  2183.234765622958\n"
     ]
    }
   ],
   "source": [
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda.perplexity(BoW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: room bar vega stay night strip nice get floor stayed great view day area club check game one clean fun\n",
      "Topic #1: pho additional paris planet vietnamese martini soooo refilled mark common broth macaron blood warned convenience jelly luggage grandma background 9pm\n",
      "Topic #2: sushi roll ayce salmon chef rice japanese fresh sashimi poke crab nigiri earl nearly miso sake order uni yellowtail piece\n",
      "Topic #3: patio gelato upgrade slider creme ravioli champagne romantic deli macaroon dj rio appetite brulee stew terrace marquee bath raised treasure\n",
      "Topic #4: soup noodle thai broth pad chicken dumpling duck pita tom jean ube messy flan satay eggslut batali chive shelf kfc\n",
      "Topic #5: cosmo de la el brie dan kimchi mai soho sora gordo monta mlife lo france mike cheek patience rum tonkotsu\n",
      "Topic #6: cream dessert ice chocolate bar sweet space course roasted beat diner green lemon cooky red pulled apple fruit juice banana\n",
      "Topic #7: burger cheese lobster sauce onion cooked potato ordered flavor pork bacon mac bun bread grilled perfectly tender beef belly side\n",
      "Topic #8: sandwich taco breakfast egg chicken cheese mexican waffle chip toast brunch bacon burrito potato ordered salsa crepe bread french nacho\n",
      "Topic #9: like place get go one food know even would bad star better say thing never really want good time review\n",
      "Topic #10: pizza pasta salad italian crust bread topping sauce meatball tomato olive spaghetti pie garlic buffalo ordered aria dough dressing pepperoni\n",
      "Topic #11: cake wine bartender birthday glass dinner sea crab anniversary dining foie gras bass chef evening cheesecake excellent pastrami carpaccio tank\n",
      "Topic #12: good food place great like time service restaurant one really also vega back menu best try go get delicious chicken\n",
      "Topic #13: hotel mgm bed standard resort bone booked everywhere company hang beautiful comp class housekeeping mr goat conference rack 17 spa\n",
      "Topic #14: al pastor da football taxi die dente hat slut halo und man pt hungover path der war elote huevos beaten\n",
      "Topic #15: vegan dog free option thru gluten cosmopolitan drive vegetarian tapa cookie wrap healthy melted continue wedding cab prepare driver mediterranean\n",
      "Topic #16: time service order food table minute back came would asked got said told took get drink wait server one went\n",
      "Topic #17: coffee tea pancake breakfast pastry downtown milk cafe bagel croissant shop cup bakery iced omelette drink morning biscuit french outlet\n",
      "Topic #18: bistro bottomless pit cutting deliciousness halibut wheel antipasto ink orzo nove karaage kyara hakkasan royal cigar daniel gambled ce bulgarian\n",
      "Topic #19: pool street sausage delivery across fremont ny ceviche city distance sin lasagna encore alcoholic promised falafel deluxe devoured within wynn\n",
      "Topic #20: los oreo diablo tenderloin midway dome branzino ipad slush reuben soufflé banchan oscar bridge appointed kebab est dill shallot rubbed\n",
      "Topic #21: buffet casino rib crab bellagio leg dessert luxor play mirage bbq section seafood short slot brisket variety indian asian vip\n",
      "Topic #22: store meet parmesan slaw hooter dress chowder coleslaw internet picasso code joe monte tomorrow cole bucket iron italy imperial puff\n",
      "Topic #23: steak beer filet center wine ribeye shopping mignon house dinner cut side east reservation main waiter oz bisque frites coast\n",
      "Topic #24: pinball poker mi banh un que chashu reduction reserve thigh float trap brian bo hen hue 2012 mizumi adobo vehicle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: macaron smoothie fruity pebble karaoke smoothy marrakech stingy pride eddie tiger samurai inevitable aradia mumbling airport damned billiards apologizing bobba\n",
      "Topic #1: enchirrito clap pant death b4 moaning discovering builder committing minna shocker kissed ahve workplace ambrosia taboo insignificant khaki murder prrrrrrrrretty\n",
      "Topic #2: mi banh bahn kinh de pupusas la que biet el dac en lee vietnam corazon carne xeo beaten pate viet\n",
      "Topic #3: stock freshest philly jackpot carton maker shade pappy 1200 van winkle quickest lvcc cheesesteak 5000 mic tailored evil northeastern appetiser\n",
      "Topic #4: ett courtney sig denna sux amarone kyra restaurangen alltså gick annunciate fajole panzenera dey paparadelle moonwalk rätt mer borde måttet\n",
      "Topic #5: manzo di plin ai tajarin tuorli urbani brisson arrosto beppino jaden parmigiano marinate cray wined translated scoff cavoletti anatra sheryl\n",
      "Topic #6: awesomeness chee jipped wirtz fleur foley hade koppa square asijah owed 4tonight tix slows alphabetical felton travelocity tudor marssa barnett\n",
      "Topic #7: growler yard hung domination goblet argument sauerkraut terry lawd jake separating souplantation kronos ian doable picnic thi skepticism entitlement 15ish\n",
      "Topic #8: steak restaurant wine meal dish lobster pie course side menu service potato dessert vega bone dinner experience delmonico best great\n",
      "Topic #9: nom hangout bye roach sensation cockroach entertainer ob 3x3 brewer keith commander eyebrow senseless taker kraken morrocon abv zane practicing\n",
      "Topic #10: participation fritto ripe gnocco nickel bombed di eataly informs encourages underground laden shortage utah frittata florida generation entourage arab morroccan\n",
      "Topic #11: organic wirtz ramsey morracan role decency mojitos goblet dem seminar echo fastfood dutchman zealous cathy stuble 1833 perfeccccct sorted patisserie\n",
      "Topic #12: dish place good great shrimp noodle taiga wow food check sauce menu roll look squid beautifully delicious simple list twilight\n",
      "Topic #13: kabob taro cha nuong hu tieu northwest tom nem authentic gio thit nam brochette cotton cuon jenny cornish vang bi\n",
      "Topic #14: ramen noodle pork spicy garlic miso rice bowl gyoza fried tonkatsu tonkotsu shoku black ale bachi ace salty japanese house\n",
      "Topic #15: och var det som jag på att den har inte lite arby för detta av man und om riktigt faktiskt\n",
      "Topic #16: lawrys mccormick fullerton summit mingnon cris rapped order sake maggianos taiga friendly time even precut bottle roll location 10 always\n",
      "Topic #17: pho roll noodle broth rice egg bowl chicken soup sauce fried bun thai dish vietnamese spicy spring restaurant meat ordered\n",
      "Topic #18: squid jenni studded hydrate withhold implementing strategic devilishly oishi cheong restoration faired expiring catalog lemongrass katsu torus frickin taiga drunken\n",
      "Topic #19: durian marrakesh horrid tum spill omelete bueno unavailable audience edna peace byron wkend submit kyle slippery wig potstickes crock tantrum\n",
      "Topic #20: beer smashburger smash groupon tap ike location rosemary indian 95 ipa palace across viet tenaya street toddy masala hemant brewing\n",
      "Topic #21: food place good like time one service get great back go really would order got even cream came also ordered\n",
      "Topic #22: chipotle curry katsu burrito macarons salsa unlv guacamole rice student height eataly argana ube cutlet id torus guava tahini college\n",
      "Topic #23: porter pearl hoppy kerala hh bamtastic acoustic konchu sung kappa colonial 6am amine margherita toddy reiterate hunch marketed malabar hawt\n",
      "Topic #24: puking postpone posioning thai flavor food dish well place lunch cod sweet japanese lot duck came combined menu would tea\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top worl for first 1000\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: chicken fry cheese waffle sauce fried bacon mac side ordered crispy potato tender slider frites bloody mary french buffalo got\n",
      "Topic #1: buffet thai crab station leg curry lamb cosmo pad paris prime seafood com asian indian bouchon leftover yelp bellagio http\n",
      "Topic #2: resort shower ravioli sub island gambling fully deli subway roti wheat eastern bun kiki poker banh moroccan dome hi treasure\n",
      "Topic #3: room get place time one like hotel go vega would bar people even night food day drink stay line great\n",
      "Topic #4: state tom flower horseradish shabu dealer gaming bath fuku bedroom poivre quote edo unassuming huevos amusement chelsea journey justin lodge\n",
      "Topic #5: chicken rice pork fried ramen spicy noodle soup sauce beef bowl dish meat shrimp broth like chinese good roll bbq\n",
      "Topic #6: tea circus filet boba milk ribeye mignon crap beet gift slut calorie dress unprofessional accompanied cuban vip monte taro facing\n",
      "Topic #7: question health answered carnevino animal concerned code hall hung shut informative remind department runner clue twenty slush unable losing gracious\n",
      "Topic #8: eggplant housekeeping slide peppercorn n9ne tank safety eiffel curtain gin kiosk playground rain spain loin tandoori manhattan rooftop forum shareable\n",
      "Topic #9: golf band marquee course sautéed booking topgolf outrageous dirt 500 bulgarian performer charcuterie curly knight extreme pong conant pajama hockey\n",
      "Topic #10: pizza pie crust italian foie spaghetti gras suite topping sauce ranch pepperoni gyro dough oven mozzarella ny ricotta delivery ingredient\n",
      "Topic #11: place great food good vega delicious service best also amazing time love definitely one restaurant menu try like back really\n",
      "Topic #12: taco mexican burrito salsa chip bean nacho asada tortilla carne margarita guacamole rice meat quesadilla sun pastor street goblet cane\n",
      "Topic #13: steakhouse delmonico coleslaw pate bark outback porterhouse 808 killed ripped mule madness pupusas cashew durango wellington lock brief halo tomahawk\n",
      "Topic #14: meatball al carson vodka da cheesesteak dente die batali raise internet pinot chix man und branzino headache mobile der war\n",
      "Topic #15: brunch pancake spent coupon champagne omelet mimosa greek hummus pita bottomless pumpkin wong showing screaming watered falafel scampi whatsoever ihop\n",
      "Topic #16: food good like time ordered place would one service came get really got order table restaurant back go even eat\n",
      "Topic #17: burger bun chili sea onion dumpling con bachi doughnut goat sweet salt brownie pro cigarette distance manor spoiled polenta pepper\n",
      "Topic #18: kid wing pho thin son spring hooter sprout adult mint tofu vietnamese child jelly mountain id brussel crispy flour barbecue\n",
      "Topic #19: manner timely squash mahi popover outdated boat puerto ale benny boulevard butternut sterling vegenation fennel souffle chilaquiles ownership robert marrakech\n",
      "Topic #20: order asked service said minute customer manager back never server came waited another finally waiting ask walked phone table guy\n",
      "Topic #21: sandwich tomato egg rude cheese turkey bacon woman lettuce wrap melt mayo onion philly pickle chipotle nasty grilled watermelon sorbet\n",
      "Topic #22: duck im picasso include cant michael dance dj karaoke everytime mina stage oreo exquisite dont confit weekly kyara overlook oj\n",
      "Topic #23: steak lobster salad dessert rib dish pasta potato sauce cake dinner cooked soup meat shrimp chocolate course side meal appetizer\n",
      "Topic #24: de sin los el emeril un celebrity que pollo en lo tear dan dave se si depressing arroz del earthy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top words for first 50\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
